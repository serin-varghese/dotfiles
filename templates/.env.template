# =============================================================================
# Environment Variables Template for Data Science Projects
# =============================================================================
# Copy this file to .env in your project root and fill in actual values
# NEVER commit the actual .env file - it should be in .gitignore
#
# Load in Python with: python-dotenv
#   from dotenv import load_dotenv
#   load_dotenv()
#   import os
#   workspace_url = os.getenv("DATABRICKS_WORKSPACE_URL")

# =============================================================================
# Databricks Configuration
# =============================================================================
DATABRICKS_WORKSPACE_URL=https://<workspace-name>.azuredatabricks.net
DATABRICKS_TOKEN=<your-personal-access-token>
DATABRICKS_CLUSTER_ID=<cluster-id>
DATABRICKS_WAREHOUSE_ID=<sql-warehouse-id>

# Databricks Connect (for local Spark development)
# SPARK_REMOTE=sc://<workspace-url>:443/;token=<token>;x-databricks-cluster-id=<cluster-id>

# =============================================================================
# Azure Authentication
# =============================================================================
AZURE_TENANT_ID=<your-tenant-id>
AZURE_SUBSCRIPTION_ID=<your-subscription-id>
AZURE_CLIENT_ID=<service-principal-client-id>
AZURE_CLIENT_SECRET=<service-principal-secret>

# Azure Resource Group
AZURE_RESOURCE_GROUP=<your-resource-group>
AZURE_LOCATION=eastus  # or your preferred region

# =============================================================================
# Azure Storage Account
# =============================================================================
AZURE_STORAGE_ACCOUNT_NAME=<storage-account-name>
AZURE_STORAGE_ACCOUNT_KEY=<storage-account-key>
AZURE_STORAGE_CONNECTION_STRING=<connection-string>

# Azure Data Lake Storage Gen2
ADLS_ACCOUNT_NAME=<adls-account-name>
ADLS_CONTAINER_NAME=<container-name>
ADLS_FILESYSTEM=<filesystem-name>

# SAS Token (if using)
AZURE_STORAGE_SAS_TOKEN=<sas-token>

# =============================================================================
# Azure Key Vault
# =============================================================================
AZURE_KEY_VAULT_NAME=<key-vault-name>
AZURE_KEY_VAULT_URL=https://<key-vault-name>.vault.azure.net/

# =============================================================================
# Azure ML / AI Services
# =============================================================================
AZURE_ML_WORKSPACE_NAME=<ml-workspace-name>
AZURE_OPENAI_ENDPOINT=https://<resource-name>.openai.azure.com/
AZURE_OPENAI_API_KEY=<api-key>
AZURE_OPENAI_DEPLOYMENT_NAME=<deployment-name>
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# =============================================================================
# Database Connections
# =============================================================================
# PostgreSQL
POSTGRES_HOST=<hostname>
POSTGRES_PORT=5432
POSTGRES_DB=<database-name>
POSTGRES_USER=<username>
POSTGRES_PASSWORD=<password>
POSTGRES_CONNECTION_STRING=postgresql://<user>:<password>@<host>:<port>/<database>

# SQL Server
SQLSERVER_HOST=<hostname>
SQLSERVER_PORT=1433
SQLSERVER_DB=<database-name>
SQLSERVER_USER=<username>
SQLSERVER_PASSWORD=<password>

# DuckDB (local)
DUCKDB_PATH=./data/database.duckdb

# =============================================================================
# MLflow Configuration
# =============================================================================
MLFLOW_TRACKING_URI=databricks
MLFLOW_EXPERIMENT_NAME=/Users/<your-email>/experiments/<project-name>
# Or for local MLflow:
# MLFLOW_TRACKING_URI=http://localhost:5000
# MLFLOW_ARTIFACT_ROOT=./mlruns

# =============================================================================
# Data Paths
# =============================================================================
DATA_DIR=./data
RAW_DATA_DIR=./data/raw
PROCESSED_DATA_DIR=./data/processed
MODELS_DIR=./models
LOGS_DIR=./logs

# Azure Blob Storage paths (using wasbs:// or abfss:// protocol)
AZURE_DATA_PATH=abfss://<container>@<account>.dfs.core.windows.net/
AZURE_RAW_DATA_PATH=abfss://<container>@<account>.dfs.core.windows.net/raw/
AZURE_PROCESSED_DATA_PATH=abfss://<container>@<account>.dfs.core.windows.net/processed/

# =============================================================================
# API Keys & External Services
# =============================================================================
# GitHub (for Databricks Repos)
GITHUB_TOKEN=<github-personal-access-token>

# Hugging Face
HUGGINGFACE_TOKEN=<hf-token>

# Weights & Biases
WANDB_API_KEY=<wandb-api-key>
WANDB_PROJECT=<project-name>
WANDB_ENTITY=<entity-name>

# OpenAI (if using OpenAI directly, not Azure)
OPENAI_API_KEY=<openai-api-key>

# =============================================================================
# Project Configuration
# =============================================================================
PROJECT_NAME=my-data-science-project
ENVIRONMENT=development  # development, staging, production
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
PYTHON_VERSION=3.11

# =============================================================================
# Spark Configuration (for local development)
# =============================================================================
SPARK_HOME=/usr/local/spark
PYSPARK_PYTHON=python3
PYSPARK_DRIVER_PYTHON=jupyter
PYSPARK_DRIVER_PYTHON_OPTS=lab

# =============================================================================
# Proxy Settings (if needed - already in .zshrc_local, but useful for CI/CD)
# =============================================================================
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1,.local

# =============================================================================
# Feature Flags / Toggles
# =============================================================================
ENABLE_DEBUGGING=false
USE_CACHE=true
PARALLEL_PROCESSING=true
MAX_WORKERS=4

# =============================================================================
# Notification Settings
# =============================================================================
SLACK_WEBHOOK_URL=<slack-webhook-url>
EMAIL_NOTIFICATIONS=your.email@example.com

# =============================================================================
# Random Seed (for reproducibility)
# =============================================================================
RANDOM_SEED=42
